# -*- coding: utf-8 -*-
"""Carl's AI Flowers 102-2023

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jdaYLwXT-nBdOuNDfV8WbIWsf8MdVTnX

# Flowers 102
"""

import matplotlib.pyplot as plt
def plot(x,title=None):
    # Move tensor to CPU and convert to numpy
    x_np = x.cpu().numpy()

    # If tensor is in (C, H, W) format, transpose to (H, W, C)
    if x_np.shape[0] == 3 or x_np.shape[0] == 1:
        x_np = x_np.transpose(1, 2, 0)

    # If grayscale, squeeze the color channel
    if x_np.shape[2] == 1:
        x_np = x_np.squeeze(2)

    x_np = x_np.clip(0, 1)

    fig, ax = plt.subplots()
    if len(x_np.shape) == 2:  # Grayscale
        im = ax.imshow(x_np, cmap='gray')
    else:
        im = ax.imshow(x_np)
    plt.title(title)
    ax.axis('off')
    fig.set_size_inches(10, 10)
    plt.show()

!wget https://gist.githubusercontent.com/JosephKJ/94c7728ed1a8e0cd87fe6a029769cde1/raw/403325f5110cb0f3099734c5edb9f457539c77e9/Oxford-102_Flower_dataset_labels.txt
!wget https://s3.amazonaws.com/content.udacity-data.com/courses/nd188/flower_data.zip
!unzip 'flower_data.zip'

import torch
from torchvision import datasets, transforms
import os
import pandas as pd

data_dir = '/content/flower_data/'
mean = [0.485, 0.456, 0.406]
std = [0.229, 0.224, 0.225]

data_transform = transforms.Compose([
    transforms.RandomResizedCrop(224),
    transforms.ToTensor(),
    transforms.Normalize(mean, std)
])

dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), data_transform)
dataset_labels = pd.read_csv('Oxford-102_Flower_dataset_labels.txt', header=None)[0].str.replace("'", "").str.strip()

dataloader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset), shuffle=False)

images, labels = next(iter(dataloader))

print(f"Images tensor shape: {images.shape}")
print(f"Labels tensor shape: {labels.shape}")

i = 50
plot(images[i],dataset_labels[i]);

i = 66
plot(images[i],dataset_labels[i]);

i = 98
plot(images[i],dataset_labels[i]);

import torch
from torchvision import models, transforms
import requests
from PIL import Image
import torch.nn.functional as F
import matplotlib.pyplot as plt

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

alexnet = models.alexnet(pretrained=True).to(device)
labels = {int(key):value for (key, value) in requests.get('https://s3.amazonaws.com/mlpipes/pytorch-quick-start/labels.json').json().items()}

#transform image for use in model
preprocess = transforms.Compose([
   transforms.Resize(256),
   transforms.CenterCrop(224),
   transforms.ToTensor(),
   transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])
])

img = images[i]

from torchvision.transforms import ToPILImage
to_pil = ToPILImage()
img = to_pil(img)

img_t = preprocess(img).unsqueeze_(0).to(device)

scores, class_idx = alexnet(img_t).max(1)
print('Predicted class:', labels[class_idx.item()])

w0 = alexnet.features[0].weight.data
w1 = alexnet.features[3].weight.data
w2 = alexnet.features[6].weight.data
w3 = alexnet.features[8].weight.data
w4 = alexnet.features[10].weight.data
w5 = alexnet.classifier[1].weight.data
w6 = alexnet.classifier[4].weight.data
w7 = alexnet.classifier[6].weight.data

img_t[0,:,:,:].shape

def scale(img):
    max_value = img.max()
    min_value = img.min()
    normalized_array = (img - min_value) / (max_value - min_value)
    return normalized_array

def tensor_plot(img_t,index=0):
    numpy_array = img_t[index,:,:,:].cpu().numpy()
    numpy_array_transposed = numpy_array.transpose(1, 2, 0)
    numpy_array_transposed = scale(numpy_array_transposed)
    plt.imshow(numpy_array_transposed)
    plt.show()

tensor_plot(img_t)

f0 = F.conv2d(img_t, w0, stride=4, padding=2)

i = 5
plt.imshow(f0[0,i,:,:].cpu().numpy())

for i in range(64):
     tensor_plot(w0,i)
     plt.imshow(f0[0,i,:,:].cpu().numpy())
     plt.show()

import torch
import matplotlib.pyplot as plt

def plot_feature_maps_with_filters(feature_maps, filters):
    if feature_maps.dim() == 4:
        feature_maps = feature_maps.squeeze(0)
    feature_maps = (feature_maps - feature_maps.min()) / (feature_maps.max() - feature_maps.min())

    def add_filter_to_feature_map(filter_tensor, feature_map_tensor):
        if feature_map_tensor.dim() > 2:
            feature_map_tensor = feature_map_tensor.squeeze(0)

        feature_map_rgb = feature_map_tensor.unsqueeze(0).repeat((3, 1, 1))

        filter_tensor = (filter_tensor - filter_tensor.min()) / (filter_tensor.max() - filter_tensor.min())

        min_dim = min(feature_map_tensor.shape)
        filter_size = min(filter_tensor.shape[-1], min_dim)

        filter_cropped = filter_tensor[:, :filter_size, :filter_size]

        feature_map_rgb[:, -filter_size:, :filter_size] = filter_cropped

        feature_map_rgb = torch.clamp(feature_map_rgb, 0, 1)

        return feature_map_rgb

    fig, axes = plt.subplots(8, 8, figsize=(15, 15))

    for ax, feature_map, filter_ in zip(axes.flat, feature_maps, filters):
        modified_feature_map = add_filter_to_feature_map(filter_, feature_map)

        ax.imshow(modified_feature_map.permute(1, 2, 0).cpu().numpy(), interpolation='none')
        ax.axis('off')

    plt.show()

plot_feature_maps_with_filters(f0, w0)